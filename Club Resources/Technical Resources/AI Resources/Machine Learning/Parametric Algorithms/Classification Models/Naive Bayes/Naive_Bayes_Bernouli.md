# Naive Bayes (Bernoulli) in Machine Learning

## Introduction
Bernoulli Naive Bayes is a variant of the Naive Bayes classification algorithm designed for **binary/boolean features**. It is a probabilistic classifier based on Bayes' theorem and the "naive" assumption of conditional independence between features. While the Multinomial variant works with word counts, the Bernoulli model works with word presence or absence, making it particularly effective for tasks like text classification with short documents (e.g., tweets, headlines) where the occurrence of a word is more important than its frequency.

## The Bernoulli Naive Bayes Model
The model represents documents as binary feature vectors. For a given vocabulary of words, each document is a vector of 0s and 1s, where a `1` indicates the presence of a word from the vocabulary in the document, and a `0` indicates its absence.

### 1. The Likelihood
Bernoulli Naive Bayes assumes that each feature $x_i$ (representing the presence/absence of word $i$) is generated by a **Bernoulli distribution**. The probability of observing feature $x_i$ given a class $C_k$ is:

$$P(x_i|C_k) = p_{ki}^{x_i} (1 - p_{ki})^{1 - x_i}$$

Where:
-   $x_i$ is either 1 (present) or 0 (absent).
-   $p_{ki} = P(x_i=1|C_k)$ is the probability that word $i$ is present in a document of class $C_k$.
-   $1 - p_{ki} = P(x_i=0|C_k)$ is the probability that word $i$ is absent from a document of class $C_k$.

The key difference from the Multinomial model is that Bernoulli Naive Bayes explicitly models the **absence** of features. The likelihood of a document is a product of probabilities for **all words in the vocabulary**, whether they are present or not.

### 2. Parameter Estimation and Smoothing
The parameters $p_{ki}$ are estimated from the training data using **Laplace smoothing** to avoid zero probabilities:

$$p_{ki} = \frac{\text{Number of documents in class } C_k \text{ containing word } i + \alpha}{\text{Total number of documents in class } C_k + 2\alpha}$$

The smoothing parameter $\alpha$ is typically set to 1.

### 3. The Classification Rule
The prediction for a new document is the class $C_k$ that maximizes the posterior probability (the MAP rule):

$$\text{Prediction} = \arg\max_{C_k} \left( P(C_k) \prod_{i=1}^{V} P(x_i|C_k) \right)$$

where the product is taken over the entire vocabulary of size $V$.

## Example: Spam Filtering
Let our vocabulary be {free, money, urgent}. We want to classify a new email that contains "free" and "money" but **not** "urgent".

From our training data (after smoothing), we have:
-   **Priors**: $P(\text{Spam}) = 0.4$, $P(\text{Not Spam}) = 0.6$
-   **Likelihoods for Spam**:
    -   $P(\text{free}|S) = 0.8 \implies P(\text{not free}|S) = 0.2$
    -   $P(\text{money}|S) = 0.7 \implies P(\text{not money}|S) = 0.3$
    -   $P(\text{urgent}|S) = 0.5 \implies P(\text{not urgent}|S) = 0.5$
-   **Likelihoods for Not Spam**:
    -   $P(\text{free}|NS) = 0.1 \implies P(\text{not free}|NS) = 0.9$
    -   $P(\text{money}|NS) = 0.05 \implies P(\text{not money}|NS) = 0.95$
    -   $P(\text{urgent}|NS) = 0.01 \implies P(\text{not urgent}|NS) = 0.99$

**Calculate the score for each class:**
The new document's feature vector is (free=1, money=1, urgent=0).

-   **Score(Spam)**: $P(S) \times P(\text{free}|S) \times P(\text{money}|S) \times P(\text{not urgent}|S)$
    $= 0.4 \times 0.8 \times 0.7 \times 0.5 = \mathbf{0.112}$

-   **Score(Not Spam)**: $P(NS) \times P(\text{free}|NS) \times P(\text{money}|NS) \times P(\text{not urgent}|NS)$
    $= 0.6 \times 0.1 \times 0.05 \times 0.99 = \mathbf{0.00297}$

Since Score(Spam) > Score(Not Spam), the email is classified as **Spam**.

## Bernoulli vs. Multinomial Naive Bayes
| Feature | **Bernoulli Naive Bayes** | **Multinomial Naive Bayes** |
|---|---|---|
| **Input Data** | Binary feature vectors (presence/absence) | Integer feature vectors (counts/frequencies) |
| **Modeling** | Models the presence/absence of every word in the vocabulary. | Models the frequency of words present in the document. |
| **Document Length**| Often performs better on short documents (e.g., tweets). | Often performs better on longer documents. |
| **Sparsity** | Explicitly penalizes the absence of evidence. | Ignores words that are not present. |


## Properties
-   **Probabilistic Nature**: Outputs a clear posterior probability for each class.
-   **Independence Assumption**: Assumes all features are conditionally independent given the class.
-   **Binary Features**: Specifically designed for binary or boolean feature data.
-   **Efficiency**: Extremely fast to train and make predictions, even with high-dimensional data.

## Applications
Bernoulli Naive Bayes is particularly effective when the presence of a feature is more informative than its frequency.
-   **Text Classification**: A popular choice for spam filtering and sentiment analysis, especially on short texts like social media posts, headlines, or product reviews.
-   **Medical Diagnosis**: Classifying a patient's condition based on the presence or absence of a checklist of symptoms.
-   **Fraud Detection**: Identifying fraudulent transactions based on a set of binary flags or indicators.