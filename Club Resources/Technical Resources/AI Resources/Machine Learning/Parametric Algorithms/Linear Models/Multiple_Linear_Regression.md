# Multiple Linear Regression

Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between a dependent variable and multiple independent variables. It extends simple linear regression by incorporating more than one predictor to better understand and predict the outcome.

## Definition

The general form of the MLR equation is:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n + \epsilon 
$$

Where:
- $Y$ is the dependent variable.
- $X_1, X_2, \dots, X_n$ are independent variables.
- $\beta_0$ is the intercept.
- $\beta_1, \beta_2, \dots, \beta_n$ are coefficients for each independent variable.
- $\epsilon$ is the error term.

The goal is to estimate the coefficients that minimize the sum of squared errors:

$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \dots + \hat{\beta}_n X_n
$$

## Example

## Predicting House Prices Using Multiple Linear Regression

We are using three features:  
- Size (in square feet)  
- Number of bedrooms  
- Location score

---

### üìä Data:

| House | Size (sqft) | Bedrooms | Location Score | Price ($) |
|-------|-------------|----------|----------------|-----------|
| 1     | 2000        | 3        | 85             | 450,000   |
| 2     | 1800        | 2        | 70             | 400,000   |
| 3     | 2200        | 4        | 90             | 500,000   |

---

### üìå Model Setup

We define our model as:

$$
\text{Price} = \beta_0 + \beta_1(\text{Size}) + \beta_2(\text{Bedrooms}) + \beta_3(\text{Location}) + \epsilon
$$

We‚Äôll estimate the coefficients $\beta$ using the **normal equation**:

$$
\boldsymbol{\beta} = (X^T X)^{-1} X^T y
$$

---

### üßÆ Step-by-Step Matrix Computation

Let‚Äôs define:

$$
X =
\begin{bmatrix}
1 & 2000 & 3 & 85 \\
1 & 1800 & 2 & 70 \\
1 & 2200 & 4 & 90 \\
\end{bmatrix}
,\quad
y =
\begin{bmatrix}
450000 \\
400000 \\
500000 \\
\end{bmatrix}
$$

Now compute:

$$
X^T X =
\begin{bmatrix}
3 & 6000 & 9 & 245 \\
6000 & 10840000 & 18000 & 492500 \\
9 & 18000 & 29 & 735 \\
245 & 492500 & 735 & 20425 \\
\end{bmatrix}
$$

$$
X^T y =
\begin{bmatrix}
1350000 \\
2700000000 \\
4050000 \\
111500000 \\
\end{bmatrix}
$$

Now compute:

$$
\boldsymbol{\beta} = (X^T X)^{-1} X^T y
$$

(This part typically requires numerical computation, but we‚Äôll summarize the result.)

---

### ‚úÖ Final Coefficients (Approximate, via software)

$$
\beta_0 \approx 50000,\quad \beta_1 \approx 100,\quad \beta_2 \approx 25000,\quad \beta_3 \approx 1000
$$

---

### üè† Prediction Example

**Predict price of a house** with:  
- Size = 2100 sqft  
- Bedrooms = 3  
- Location = 80  

$$
\text{Price} = 50000 + 100 \cdot 2100 + 25000 \cdot 3 + 1000 \cdot 80 = 50000 + 210000 + 75000 + 80000 = 415000
$$

---

### üìå Interpretation:

- Every additional **sqft** adds ‚âà $100  
- Every additional **bedroom** adds ‚âà $25,000  
- Every additional **location point** adds ‚âà $1,000

---


## Properties

1. **Linearity**: Relationship between Y and each X is linear.
   - Equation: $$E(Y|X) = \beta_0 + \beta_1 X_1 + \dots + \beta_n X_n$$

2. **Independence**: Errors are uncorrelated with predictors.

3. **Homoscedasticity**: Constant variance of errors across all levels of Y.

4. **Normality**: Errors follow a normal distribution, especially for hypothesis testing.

5. **Multicollinearity**: High correlation among independent variables can inflate standard errors.

6. **R-squared (R¬≤)**: Proportion of variance explained by the model.
   - Formula: $R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}$

7. **Adjusted R-squared**: Adjusts for model complexity.
   - Formula: $R^2_{\text{adj}} = 1 - \frac{n-1}{n-p-1} (1 - R^2)$

8. **F-test**: Assesses overall model significance.

## Applications

MLR is widely used in:
- **Economics**: Analyzing factors influencing GDP, inflation.
- **Finance**: Portfolio analysis, risk assessment.
- **Machine Learning**: Foundation for algorithms like Support Vector Machines.
- **Healthcare**: Predicting patient outcomes based on various factors.
- **Marketing**: Customer segmentation and demand forecasting.
