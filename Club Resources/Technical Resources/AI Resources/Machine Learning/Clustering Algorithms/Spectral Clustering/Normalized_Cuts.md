
# Normalized Cuts in Machine Learning

## Introduction
**Normalized Cuts (N-Cuts)** is a powerful graph partitioning algorithm widely used for data clustering, with its most famous application being in **image segmentation**. Introduced by Jianbo Shi and Jitendra Malik, the N-Cuts method addresses a major flaw in simpler graph cutting algorithms by creating partitions that are both meaningful and balanced. It is a foundational algorithm in the field of **spectral clustering**.

## The Problem with Simple Graph Cuts
A simple way to partition a graph is to find the **Minimum Cut**â€”the set of edges with the minimum total weight that, when removed, splits the graph into two disjoint sets. However, this approach often produces poor results.
-   **The Flaw**: The min-cut algorithm has a bias for creating very unbalanced partitions. It frequently isolates a single, outlier node from the rest of the graph, as this often requires cutting only a few, low-weight edges. This is rarely a useful or meaningful cluster.

## The Normalized Cut Criterion
The N-Cuts algorithm improves upon this by **normalizing** the cut based on the "volume" of each partition.
-   **The Goal**: To find a partition that not only has a small connection between the two groups but also ensures that the groups themselves are cohesive and reasonably large.

We define two key terms for a partition of a graph into two sets, A and B:
1.  **`cut(A, B)`**: The sum of the weights of all edges that connect a node in A to a node in B.
2.  **`vol(A)`**: The "volume" of set A, defined as the sum of the weights of all edges that have at least one endpoint in A.

The **Normalized Cut** objective function, which we aim to minimize, is:
$$Ncut(A,B) = \frac{cut(A,B)}{vol(A)} + \frac{cut(A,B)}{vol(B)}$$
This objective penalizes partitions where one of the sets (e.g., A) has a very small volume, as this would make the denominator `vol(A)` small and the overall N-Cut value large. This encourages the creation of balanced, stable partitions.

## Solving the N-Cut Problem via Spectral Methods
Finding the exact partition that minimizes the N-Cut objective is an NP-hard problem. The key breakthrough of the N-Cuts algorithm is the realization that this difficult discrete problem can be relaxed and efficiently approximated by solving a **generalized eigenvalue problem** from spectral graph theory.

The algorithm is as follows:

1.  **Represent Data as a Graph**: Given a set of data points, construct a weighted graph where the nodes are the data points and the edge weight $w_{ij}$ between two nodes represents the similarity between them.

2.  **Construct Matrices**: From the graph, create two matrices:
    -   The **Adjacency Matrix ($W$)**: A symmetric matrix where $W_{ij} = w_{ij}$.
    -   The **Degree Matrix ($D$)**: A diagonal matrix where each diagonal element $D_{ii}$ is the sum of all edge weights connected to node $i$ ($D_{ii} = \sum_j W_{ij}$).

3.  **Solve the Eigenvalue Problem**: The N-Cut problem can be approximately solved by solving the generalized eigenvalue system:

$$(D-W)\mathbf{y} = \lambda D \mathbf{y}$$

  The matrix $L = D-W$ is known as the **Graph Laplacian**.

4.  **Find the Partition**: The solution is given by the eigenvector $\mathbf{y}_2$ corresponding to the **second smallest eigenvalue**, $\lambda_2$. (The smallest eigenvalue is always 0 and its eigenvector is trivial).
   
5.  **Thresholding**: The eigenvector $\mathbf{y}_2$ is a real-valued vector. To get a discrete partition, we simply threshold its components. For example, all nodes $i$ where $y_2(i) > 0$ are assigned to cluster A, and all other nodes are assigned to cluster B. To find more than two clusters, this process can be applied recursively.

## Properties
-   **Balanced Clusters**: Its primary advantage. The normalization term in the objective function actively discourages the creation of very small, trivial clusters.
-   **Spectral Relaxation**: It provides an elegant and efficient approximate solution to an NP-hard problem by reformulating it in the context of linear algebra (eigenvectors).
-   **Computational Cost**: Its main drawback is the high computational cost. The bottleneck is solving the eigenvalue problem, which is roughly $O(n^3)$ for an $n \times n$ matrix. This makes it unsuitable for very large graphs.
-   **Finds Non-Convex Shapes**: As a spectral method, it can identify clusters with complex, non-convex shapes.

## Applications
1.  **Image Segmentation**: This is the canonical application. Pixels are treated as nodes in a graph, and edge weights are based on the similarity of nearby pixels (in terms of color, brightness, and texture). N-Cuts can find the boundaries between perceptually distinct regions in the image, effectively separating an object from its background.
2.  **Social Network Analysis**: To identify communities within a social network. The algorithm can find balanced partitions of the network graph, revealing distinct groups of users who are more strongly connected to each other than to the rest of the network.
3.  **Bioinformatics**: For clustering genes based on expression data or analyzing protein interaction networks to find functional modules.
4.  **Mesh Partitioning**: In scientific computing and engineering, for partitioning a complex mesh (e.g., for a finite element simulation) into balanced subdomains to be processed in parallel.